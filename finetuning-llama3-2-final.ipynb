{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12847534,"sourceType":"datasetVersion","datasetId":8125889}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/rishithreddyabcdef/finetuning-llama3-2-final?scriptVersionId=258203285\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install trl==0.12.2\n!pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu118\n!pip install -U \"huggingface_hub[cli]\"\n!pip install transformers==4.46.3\n!pip install accelerate==1.10.0\n!pip install sentence-transformers==3.3.1\n!pip install peft==0.9.0\n!pip install numpy==1.26.4\n!pip install evaluate\n!pip install seqeval \n!pip install bitsandbytes==0.46.1\n!pip install -U deepspeed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T17:41:25.279708Z","iopub.execute_input":"2025-08-24T17:41:25.279954Z","iopub.status.idle":"2025-08-24T17:46:14.462485Z","shell.execute_reply.started":"2025-08-24T17:41:25.279934Z","shell.execute_reply":"2025-08-24T17:46:14.461795Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom datasets import Dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    Trainer,\n    PreTrainedModel,\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom seqeval.metrics import classification_report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T15:31:36.21957Z","iopub.execute_input":"2025-08-25T15:31:36.219831Z","iopub.status.idle":"2025-08-25T15:32:03.636875Z","shell.execute_reply.started":"2025-08-25T15:31:36.219813Z","shell.execute_reply":"2025-08-25T15:32:03.635862Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T15:32:03.637292Z","iopub.status.idle":"2025-08-25T15:32:03.637532Z","shell.execute_reply.started":"2025-08-25T15:32:03.637421Z","shell.execute_reply":"2025-08-25T15:32:03.637431Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\nSEED = 42\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T17:49:16.404801Z","iopub.execute_input":"2025-08-24T17:49:16.405061Z","iopub.status.idle":"2025-08-24T17:49:16.413055Z","shell.execute_reply.started":"2025-08-24T17:49:16.405041Z","shell.execute_reply":"2025-08-24T17:49:16.412352Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def conll_to_sentence_tags(filepath):\n    sentences = []\n    tags = []\n    cur_tokens, cur_tags = [], []\n    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                if cur_tokens:\n                    sentences.append(cur_tokens)\n                    tags.append(cur_tags)\n                    cur_tokens, cur_tags = [], []\n                continue\n            if line.startswith(\"#\"):\n                continue\n            parts = line.split(\"\\t\")\n            if len(parts) >= 4:\n                token, upos = parts[1], parts[3]\n                cur_tokens.append(token)\n                cur_tags.append(upos)\n    if cur_tokens:\n        sentences.append(cur_tokens)\n        tags.append(cur_tags)\n    return pd.DataFrame({\"sentence\": sentences, \"tags\": tags})\ntrain_df = conll_to_sentence_tags(r\"/kaggle/input/conll-tel/te_mtg-ud-train.conllu\")\ntest_df  = conll_to_sentence_tags(r\"/kaggle/input/conll-tel/te_mtg-ud-test.conllu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T17:49:16.88606Z","iopub.execute_input":"2025-08-24T17:49:16.886595Z","iopub.status.idle":"2025-08-24T17:49:16.934696Z","shell.execute_reply.started":"2025-08-24T17:49:16.886569Z","shell.execute_reply":"2025-08-24T17:49:16.933961Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_tag_lists = list(train_df[\"tags\"]) + list(test_df[\"tags\"])\nlabel_list = sorted({str(lab).strip() for tag_list in all_tag_lists for lab in tag_list})\nlabel2id = {lab: i for i, lab in enumerate(label_list)}\nid2label = {i: lab for lab, i in label2id.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T17:49:19.311537Z","iopub.execute_input":"2025-08-24T17:49:19.311804Z","iopub.status.idle":"2025-08-24T17:49:19.323744Z","shell.execute_reply.started":"2025-08-24T17:49:19.311783Z","shell.execute_reply":"2025-08-24T17:49:19.323108Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token  # ensure padding exists\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n)\nbase = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\nbase.config.pad_token_id = tokenizer.pad_token_id\nbase = prepare_model_for_kbit_training(base)\nlora_cfg = LoraConfig(\n    r=12,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"TOKEN_CLS\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T17:51:48.942609Z","iopub.execute_input":"2025-08-24T17:51:48.942884Z","iopub.status.idle":"2025-08-24T17:51:48.947127Z","shell.execute_reply.started":"2025-08-24T17:51:48.942863Z","shell.execute_reply":"2025-08-24T17:51:48.946579Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LlamaForTokenClassification(PreTrainedModel):\n    def __init__(self, peft_causal_model, num_labels, id2label, label2id, class_weights=None):\n        config = peft_causal_model.base_model.config\n        super().__init__(config)\n        self.peft_causal = peft_causal_model\n        self.num_labels = num_labels\n        hidden = config.hidden_size\n        self.dropout = nn.Dropout(0.1)\n        self.classifier = nn.Linear(hidden, num_labels)\n        self.config.id2label = id2label\n        self.config.label2id = label2id\n        # store as buffer so it moves with .to(device)\n        if class_weights is not None:\n            self.register_buffer(\"class_weights\", class_weights)\n        else:\n            self.class_weights = None\n    def forward(self, input_ids=None, attention_mask=None, labels=None):\n        outputs = self.peft_causal(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            use_cache=False,\n            output_hidden_states=True,\n            return_dict=True,\n        )\n        last_hidden = outputs.hidden_states[-1]  \n        x = self.dropout(last_hidden)\n        logits = self.classifier(x)             \n        loss = None\n        if labels is not None:\n            if self.class_weights is not None:\n                loss_fct = nn.CrossEntropyLoss(ignore_index=-100, weight=self.class_weights)\n            else:\n                loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n\n        return {\"loss\": loss, \"logits\": logits}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T17:51:55.664035Z","iopub.execute_input":"2025-08-24T17:51:55.664598Z","iopub.status.idle":"2025-08-24T17:51:55.671518Z","shell.execute_reply.started":"2025-08-24T17:51:55.664573Z","shell.execute_reply":"2025-08-24T17:51:55.670863Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_class_weights(train_dataset, num_labels, ignore_index=-100, smoothing=1.0):\n    counts = np.zeros(num_labels, dtype=np.float64)\n    for row in train_dataset[\"labels\"]:\n        for lid in row:\n            if lid != ignore_index:\n                counts[lid] += 1\n    weights = 1.0 / (counts + smoothing)\n    weights = weights * (num_labels / weights.sum())\n    return torch.tensor(weights, dtype=torch.float32)\nMAX_LEN = 128\ndef prepare_dataset(df, tokenizer, label2id, max_length=128):\n    df = df.copy()\n    df[\"tokens\"] = df[\"sentence\"].apply(lambda x: x if isinstance(x, list) else str(x).split())\n    df[\"labels\"] = df[\"tags\"].apply(lambda x: x if isinstance(x, list) else str(x).split())\n    dataset = Dataset.from_pandas(df, preserve_index=False)\n    def tokenize_and_align_labels(examples, label_all_tokens=False):\n        tokenized = tokenizer(\n            examples[\"tokens\"],               \n            is_split_into_words=True,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=max_length,\n        )\n        all_labels = []\n        for i, labels in enumerate(examples[\"labels\"]):\n            word_ids = tokenized.word_ids(batch_index=i)\n            previous_word_id = None\n            label_ids = []\n            for word_id in word_ids:\n                if word_id is None:\n                    label_ids.append(-100)\n                elif word_id != previous_word_id:\n                    # first sub-token of this word\n                    label_ids.append(label2id[labels[word_id]])\n                else:\n                    # subsequent sub-tokens\n                    label_ids.append(label2id[labels[word_id]] if label_all_tokens else -100)\n                previous_word_id = word_id\n            all_labels.append(label_ids)\n        tokenized[\"labels\"] = all_labels\n        return tokenized\n    def _map_fn(examples):\n        out = tokenize_and_align_labels(examples, label_all_tokens=False)\n        out[\"tokens\"] = examples[\"tokens\"]\n        return out\n    cols_to_keep = [\"tokens\"]  \n    return dataset.map(\n        _map_fn,\n        batched=True,\n        remove_columns=[c for c in dataset.column_names if c not in cols_to_keep],\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T17:51:56.480938Z","iopub.execute_input":"2025-08-24T17:51:56.481481Z","iopub.status.idle":"2025-08-24T17:51:56.490011Z","shell.execute_reply.started":"2025-08-24T17:51:56.48146Z","shell.execute_reply":"2025-08-24T17:51:56.489471Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = prepare_dataset(train_df, tokenizer, label2id, max_length=MAX_LEN)\neval_dataset  = prepare_dataset(test_df,  tokenizer, label2id, max_length=MAX_LEN)\nclass_weights = compute_class_weights(train_dataset, num_labels=len(label_list))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T17:51:59.399675Z","iopub.execute_input":"2025-08-24T17:51:59.400466Z","iopub.status.idle":"2025-08-24T17:52:00.012288Z","shell.execute_reply.started":"2025-08-24T17:51:59.400441Z","shell.execute_reply":"2025-08-24T17:52:00.011548Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Num labels:\", len(label_list))\nprint(\"Labels:\", label_list)\npeft_causal = get_peft_model(base, lora_cfg)\npeft_causal.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T17:52:01.824502Z","iopub.execute_input":"2025-08-24T17:52:01.825234Z","iopub.status.idle":"2025-08-24T17:52:01.979874Z","shell.execute_reply.started":"2025-08-24T17:52:01.825207Z","shell.execute_reply":"2025-08-24T17:52:01.979202Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = LlamaForTokenClassification(\n    peft_causal_model=peft_causal,\n    num_labels=len(label_list),\n    id2label=id2label,\n    label2id=label2id,\n    class_weights=class_weights\n)\nargs = TrainingArguments(\n    output_dir=\"./lora-pos-tokencls\",\n    per_device_train_batch_size=10,\n    gradient_accumulation_steps=4,\n    num_train_epochs=9,\n    learning_rate=0.00001,\n    logging_steps=10,\n    save_steps=200,\n    eval_strategy=\"steps\",\n    eval_steps=10,\n    fp16=torch.cuda.is_available(),\n    optim='lion_32bit',\n    gradient_checkpointing=False,\n    report_to=\"tensorboard\",  \n)\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer,\n)\ntrainer.train()\ntrainer.save_model(\"./lora-pos-tokencls\")\ntokenizer.save_pretrained(\"./lora-pos-tokencls\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T18:55:35.909727Z","iopub.execute_input":"2025-08-24T18:55:35.910353Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Running prediction on eval set...\")\npreds = trainer.predict(eval_dataset)\npred_ids = preds.predictions.argmax(-1) \ntrue_labels, pred_labels = [], []\nfor i in range(len(eval_dataset)):\n    enc = tokenizer(\n        eval_dataset[i][\"tokens\"],\n        is_split_into_words=True,\n        padding=\"max_length\",\n        truncation=True,\n        max_length=MAX_LEN,\n        return_tensors=\"pt\",\n    )\n    word_ids = enc.word_ids(batch_index=0)\n    sent_true, sent_pred = [], []\n    used_words = set()\n    labels_row = eval_dataset[i][\"labels\"] if \"labels\" in eval_dataset.features else None\n    row_labels = eval_dataset[i][\"labels\"]\n    for j, wid in enumerate(word_ids):\n        if wid is None:\n            continue\n        if wid not in used_words:\n            used_words.add(wid)\n            true_id = row_labels[j]\n            if true_id != -100:\n                sent_true.append(id2label[int(true_id)])\n                sent_pred.append(id2label[int(pred_ids[i][j])])\n\n    true_labels.append(sent_true)\n    pred_labels.append(sent_pred)\nprint(classification_report(true_labels, pred_labels, digits=4))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\ny_true = [l for seq in true_labels for l in seq]\ny_pred = [l for seq in pred_labels for l in seq]\nactive_labels = [lab for lab in label_list if lab in set(y_true)]\ncm = confusion_matrix(y_true, y_pred, labels=active_labels)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=active_labels)\nplt.figure(figsize=(10, 8))\ndisp.plot(xticks_rotation=90, values_format=\"d\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rows = []\nfor tokens, gold, pred in zip(eval_dataset[\"tokens\"], true_labels, pred_labels):\n    rows.append({\n        \"sentence\": \" \".join(tokens),\n        \"actual tags\": \"[\" + \", \".join(gold) + \"]\",\n        \"predicted tags\": \"[\" + \", \".join(pred) + \"]\",\n    })\nout_df = pd.DataFrame(rows)\nout_path = \"pos_predictions.xlsx\"\nout_df.to_excel(out_path, index=False)\nprint(f\"Predictions saved to {out_path}\")\nfor k in range(min(5, len(rows))):\n    print(\n        \"\\nInput:\", rows[k][\"sentence\"],\n        \"\\nActual:\", rows[k][\"actual tags\"],\n        \"\\nOutput:\", rows[k][\"predicted tags\"]\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Running prediction on train set...\")\npreds = trainer.predict(train_dataset)\npred_ids = preds.predictions.argmax(-1) \ntrue_labels, pred_labels = [], []\nfor i in range(len(train_dataset)):\n    enc = tokenizer(\n        train_dataset[i][\"tokens\"],\n        is_split_into_words=True,\n        padding=\"max_length\",\n        truncation=True,\n        max_length=MAX_LEN,\n        return_tensors=\"pt\",\n    )\n    word_ids = enc.word_ids(batch_index=0)\n    sent_true, sent_pred = [], []\n    used_words = set()\n    labels_row = train_dataset[i][\"labels\"] if \"labels\" in train_dataset.features else None\n    row_labels = train_dataset[i][\"labels\"]\n    for j, wid in enumerate(word_ids):\n        if wid is None:\n            continue\n        if wid not in used_words:\n            used_words.add(wid)\n            true_id = row_labels[j]\n            if true_id != -100:\n                sent_true.append(id2label[int(true_id)])\n                sent_pred.append(id2label[int(pred_ids[i][j])])\n\n    true_labels.append(sent_true)\n    pred_labels.append(sent_pred)\nprint(classification_report(true_labels, pred_labels, digits=4))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\ny_true = [l for seq in true_labels for l in seq]\ny_pred = [l for seq in pred_labels for l in seq]\nactive_labels = [lab for lab in label_list if lab in set(y_true)]\ncm = confusion_matrix(y_true, y_pred, labels=active_labels)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=active_labels)\nplt.figure(figsize=(10, 8))\ndisp.plot(xticks_rotation=90, values_format=\"d\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import pipeline\npipe = pipeline(\n    \"text-generation\",\n    model=base,\n    tokenizer=tokenizer,\n    device_map=\"auto\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"system_prompt = (\n    \"\"\"Identify following POS tags for the given sentence including punctuation \"\n    \"with only these tags ADJ, ADP, ADV, CCONJ, DET, NOUN, NUM, PART, PRON, \"\n    \"PROPN, PUNCT, SCONJ, VERB. Output only tags for respective words without explanation.\n    JUST GIVE IN THIS PROMPT \n    TAGS: Respective_tag for respective_word\"\"\"\n\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_tags(sentence: str):\n    # num_words = len(sentence.split())\n    messages = [ {\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": sentence}, ] \n    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    outputs = pipe(prompt, max_new_tokens=256, do_sample=False) \n    return outputs[0][\"generated_text\"][len(prompt):].strip()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt_test=test_df.copy(deep=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt_test['predict_tags'] = prompt_test['sentence'].apply(lambda x: predict_tags(' '.join(x[0])))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt_test","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predict_tags('చూసేరండీ ?')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt_test.to_excel('prompt.xlsx')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}